{"cells":[{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T18:19:11.412574Z","iopub.status.busy":"2020-12-13T18:19:11.411928Z","iopub.status.idle":"2020-12-13T18:19:19.878351Z","shell.execute_reply":"2020-12-13T18:19:19.87727Z"},"papermill":{"duration":8.489345,"end_time":"2020-12-13T18:19:19.878461","exception":false,"start_time":"2020-12-13T18:19:11.389116","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from time import time\nfrom datetime import timedelta\nfrom copy import deepcopy\n\nimport random\nimport numpy as np\nimport pandas as pd\nfrom ml_metrics import mapk\n\nimport torch\nfrom torch.optim import AdamW\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizerFast, BertForMultipleChoice\n\n# Random seed\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# CUDA device\nuse_cuda_device = 0\n#torch.cuda.set_device(use_cuda_device)\nprint(\"Using CUDA device: %d\" % torch.cuda.current_device())","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.013661,"end_time":"2020-12-13T18:19:19.906422","exception":false,"start_time":"2020-12-13T18:19:19.892761","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Settings"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T18:19:19.941489Z","iopub.status.busy":"2020-12-13T18:19:19.939692Z","iopub.status.idle":"2020-12-13T18:19:19.942101Z","shell.execute_reply":"2020-12-13T18:19:19.942515Z"},"papermill":{"duration":0.022573,"end_time":"2020-12-13T18:19:19.942613","exception":false,"start_time":"2020-12-13T18:19:19.92004","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Input files\ndocument_csv_path = '../input/ntust-ir2020-homework6/documents.csv'\ntraining_csv_path = '../input/ntust-ir2020-homework6/train_queries.csv'\ntesting_csv_path = '../input/ntust-ir2020-homework6/test_queries.csv'\n\n# Input limitation\nmax_query_length = 64\nmax_input_length = 512\nnum_negatives = 3   # num. of negative documents to pair with a positive document\n\n# Model finetuning\nmodel_name_or_path = \"bert-base-uncased\"\nmax_epochs = 3\nlearning_rate = 3e-5\ndev_set_ratio = 0.2   # make a ratio of training set as development set for rescoring weight sniffing\nmax_patience = 0      # earlystop if avg. loss on development set doesn't decrease for num. of epochs\nbatch_size = 2    # num. of inputs = 8 requires ~9200 MB VRAM (num. of inputs = batch_size * (num_negatives + 1))\nnum_workers = 2   # num. of jobs for pytorch dataloader\n\n# Save paths\nsave_model_path = \"models/bert_base_uncased\"  # assign `None` for not saving the model\nsave_submission_path = \"bm25_bert_rescoring.csv\"\nK = 1000   # for MAP@K","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.01338,"end_time":"2020-12-13T18:19:19.96968","exception":false,"start_time":"2020-12-13T18:19:19.9563","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Preparing"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T18:19:20.007043Z","iopub.status.busy":"2020-12-13T18:19:20.00646Z","iopub.status.idle":"2020-12-13T18:19:38.305431Z","shell.execute_reply":"2020-12-13T18:19:38.304774Z"},"papermill":{"duration":18.322455,"end_time":"2020-12-13T18:19:38.305619","exception":false,"start_time":"2020-12-13T18:19:19.983164","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Build and save BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(model_name_or_path)\nif save_model_path is not None:\n    save_tokenizer_path = \"%s/tokenizer\" % (save_model_path)\n    tokenizer.save_pretrained(save_tokenizer_path)\n\n# Collect mapping of all document id and text\ndoc_id_to_text = {}\ndoc_df = pd.read_csv(document_csv_path)\ndoc_df.fillna(\"<Empty Document>\", inplace=True)\nid_text_pair = zip(doc_df[\"doc_id\"], doc_df[\"doc_text\"])\nfor i, pair in enumerate(id_text_pair, start=1):\n    doc_id, doc_text = pair\n    doc_id_to_text[doc_id] = doc_text\n    \n    print(\"Progress: %d/%d\\r\" % (i, len(doc_df)), end='')\n    \ndoc_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.129222,"end_time":"2020-12-13T18:19:38.573906","exception":false,"start_time":"2020-12-13T18:19:38.444684","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Training"},{"metadata":{"papermill":{"duration":0.125402,"end_time":"2020-12-13T18:19:38.823896","exception":false,"start_time":"2020-12-13T18:19:38.698494","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Split a ratio of training set as development set"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T18:19:39.07757Z","iopub.status.busy":"2020-12-13T18:19:39.076961Z","iopub.status.idle":"2020-12-13T18:19:39.154725Z","shell.execute_reply":"2020-12-13T18:19:39.155213Z"},"papermill":{"duration":0.207816,"end_time":"2020-12-13T18:19:39.155337","exception":false,"start_time":"2020-12-13T18:19:38.947521","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(training_csv_path)\ndev_df, train_df = np.split(train_df, [int(dev_set_ratio*len(train_df))])\ndev_df.reset_index(drop=True, inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\nprint(\"train_df shape:\", train_df.shape)\nprint(\"dev_df shape:\", dev_df.shape)\ntrain_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.122661,"end_time":"2020-12-13T18:19:39.399186","exception":false,"start_time":"2020-12-13T18:19:39.276525","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Build instances for training/development set"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T18:19:39.663683Z","iopub.status.busy":"2020-12-13T18:19:39.662944Z","iopub.status.idle":"2020-12-13T18:21:46.494246Z","shell.execute_reply":"2020-12-13T18:21:46.494717Z"},"papermill":{"duration":126.973732,"end_time":"2020-12-13T18:21:46.494868","exception":false,"start_time":"2020-12-13T18:19:39.521136","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"%%time\ndoc_id_to_token_ids = {}\ndef preprocess_df(df):\n    ''' Preprocess DataFrame into training instances for BERT. '''\n    instances = []\n    \n    # Parse CSV\n    for i, row in df.iterrows():\n        query_id, query_text, pos_doc_ids, bm25_top1000, _ = row\n        pos_doc_id_list = pos_doc_ids.split()\n        pos_doc_id_set = set(pos_doc_id_list)\n        bm25_top1000_list = bm25_top1000.split()\n        bm25_top1000_set = set(bm25_top1000_list)\n\n        # Pair BM25 neg. with pos. samples\n        labeled_pos_neg_list = []\n        for pos_doc_id in pos_doc_id_list:\n            neg_doc_id_set = bm25_top1000_set - pos_doc_id_set\n            neg_doc_ids = random.sample(neg_doc_id_set, num_negatives)\n            pos_position = random.randint(0, num_negatives)\n            pos_neg_doc_ids = neg_doc_ids\n            pos_neg_doc_ids.insert(pos_position, pos_doc_id)\n            labeled_sample = (pos_neg_doc_ids, pos_position)\n            labeled_pos_neg_list.append(labeled_sample)\n            \n        # Make query tokens for BERT\n        query_tokens = tokenizer.tokenize(query_text)\n        if len(query_tokens) > max_query_length:  # truncation\n            query_tokens = query_tokens[:max_query_length]\n        query_token_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n        query_token_ids.insert(0, tokenizer.cls_token_id)\n        query_token_ids.append(tokenizer.sep_token_id)\n\n        # Make input instances for all query/doc pairs\n        for doc_ids, label in labeled_pos_neg_list:\n            paired_input_ids = []\n            paired_attention_mask = []\n            paired_token_type_ids = []\n            \n            # Merge all pos/neg inputs as a single sample\n            for doc_id in doc_ids:\n                if doc_id in doc_id_to_token_ids:\n                    doc_token_ids = doc_id_to_token_ids[doc_id]\n                else:\n                    doc_text = doc_id_to_text[doc_id]\n                    doc_tokens = tokenizer.tokenize(doc_text)\n                    doc_token_ids = tokenizer.convert_tokens_to_ids(doc_tokens)\n                    doc_id_to_token_ids[doc_id] = doc_token_ids\n                doc_token_ids.append(tokenizer.sep_token_id)\n\n                # make input sequences for BERT\n                input_ids = query_token_ids + doc_token_ids\n                token_type_ids = [0 for token_id in query_token_ids]\n                token_type_ids.extend(1 for token_id in doc_token_ids)\n                if len(input_ids) > max_input_length:  # truncation\n                    input_ids = input_ids[:max_input_length]\n                    token_type_ids = token_type_ids[:max_input_length]\n                attention_mask = [1 for token_id in input_ids]\n                \n                # convert and collect inputs as tensors\n                input_ids = torch.LongTensor(input_ids)\n                attention_mask = torch.FloatTensor(attention_mask)\n                token_type_ids = torch.LongTensor(token_type_ids)\n                paired_input_ids.append(input_ids)\n                paired_attention_mask.append(attention_mask)\n                paired_token_type_ids.append(token_type_ids)\n            label = torch.LongTensor([label]).squeeze()\n            \n            # Pre-pad tensor pairs for efficiency\n            paired_input_ids = pad_sequence(paired_input_ids, batch_first=True)\n            paired_attention_mask = pad_sequence(paired_attention_mask, batch_first=True)\n            paired_token_type_ids = pad_sequence(paired_token_type_ids, batch_first=True)\n\n            # collect all inputs as a dictionary\n            instance = {}\n            instance['input_ids'] = paired_input_ids.T  # transpose for code efficiency\n            instance['attention_mask'] = paired_attention_mask.T\n            instance['token_type_ids'] = paired_token_type_ids.T\n            instance['label'] = label\n            instances.append(instance)\n\n        print(\"Progress: %d/%d\\r\" % (i+1, len(df)), end='')\n    print()\n    return instances\n\ntrain_instances = preprocess_df(train_df)\ndev_instances = preprocess_df(dev_df)\n\nprint(\"num. train_instances: %d\" % len(train_instances))\nprint(\"num. dev_instances: %d\" % len(dev_instances))\nprint(\"input_ids.T shape:\", train_instances[0]['input_ids'].T.shape)\ntrain_instances[0]['input_ids'].T","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.149598,"end_time":"2020-12-13T18:21:46.793926","exception":false,"start_time":"2020-12-13T18:21:46.644328","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Build dataset and dataloader for PyTorch"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T18:21:47.101505Z","iopub.status.busy":"2020-12-13T18:21:47.100915Z","iopub.status.idle":"2020-12-13T18:21:47.313632Z","shell.execute_reply":"2020-12-13T18:21:47.314639Z"},"papermill":{"duration":0.372856,"end_time":"2020-12-13T18:21:47.314771","exception":false,"start_time":"2020-12-13T18:21:46.941915","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class TrainingDataset(Dataset):\n    def __init__(self, instances):\n        self.instances = instances\n    \n    def __len__(self):\n        return len(self.instances)\n        \n    def __getitem__(self, i):\n        instance = self.instances[i]\n        input_ids = instance['input_ids']\n        attention_mask = instance['attention_mask']\n        token_type_ids = instance['token_type_ids']\n        label = instance['label']\n        return input_ids, attention_mask, token_type_ids, label\n    \ndef get_train_dataloader(instances, batch_size=2, num_workers=4):\n    def collate_fn(batch):\n        input_ids, attention_mask, token_type_ids, labels = zip(*batch)\n        input_ids = pad_sequence(input_ids, batch_first=True).transpose(1,2).contiguous()  # re-transpose\n        attention_mask = pad_sequence(attention_mask, batch_first=True).transpose(1,2).contiguous()\n        token_type_ids = pad_sequence(token_type_ids, batch_first=True).transpose(1,2).contiguous()\n        labels = torch.stack(labels)\n        return input_ids, attention_mask, token_type_ids, labels\n    \n    dataset = TrainingDataset(instances)\n    dataloader = DataLoader(dataset, collate_fn=collate_fn, shuffle=True, \\\n                            batch_size=batch_size, num_workers=num_workers)\n    return dataloader\n\n# Demo\ndataloader = get_train_dataloader(train_instances)\nfor batch in dataloader:\n    input_ids, attention_mask, token_type_ids, labels = batch\n    break\n    \nprint(input_ids.shape)\ninput_ids","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.148308,"end_time":"2020-12-13T18:21:47.611847","exception":false,"start_time":"2020-12-13T18:21:47.463539","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## Initialize and finetune BERT"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T18:21:47.923874Z","iopub.status.busy":"2020-12-13T18:21:47.923269Z","iopub.status.idle":"2020-12-13T18:22:37.828502Z","shell.execute_reply":"2020-12-13T18:22:37.827413Z"},"papermill":{"duration":50.065197,"end_time":"2020-12-13T18:22:37.828624","exception":false,"start_time":"2020-12-13T18:21:47.763427","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"model = BertForMultipleChoice.from_pretrained(model_name_or_path)\nmodel.cuda()\ntorch.cuda.empty_cache()\noptimizer = AdamW(model.parameters(), lr=learning_rate)\noptimizer.zero_grad()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.148959,"end_time":"2020-12-13T18:22:38.129229","exception":false,"start_time":"2020-12-13T18:22:37.98027","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### (TO-DO!) Define validation function for earlystopping"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T18:22:38.447243Z","iopub.status.busy":"2020-12-13T18:22:38.446575Z","iopub.status.idle":"2020-12-13T18:22:38.450378Z","shell.execute_reply":"2020-12-13T18:22:38.449875Z"},"papermill":{"duration":0.172124,"end_time":"2020-12-13T18:22:38.45049","exception":false,"start_time":"2020-12-13T18:22:38.278366","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def validate(model, instances):\n    total_loss = 0\n    model.eval()\n    dataloader = get_train_dataloader(instances, batch_size=batch_size, num_workers=num_workers)\n    for batch in dataloader:\n        batch = (tensor.cuda() for tensor in batch)\n        input_ids, attention_mask, token_type_ids, labels = batch\n        \n        ''' TO-DO: \n        1. Compute the cross-entropy loss (using built-in loss of BertForMultipleChoice)\n          (Hint: You need to call a function of model which takes all the 4 tensors in the batch as inputs)\n          \n        2. Sum up the loss of all dev-set samples\n          (Hint: The built-in loss is averaged, so you should multiply it with the batch size)\n        '''\n        with torch.no_grad():\n            outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, \n                                attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        total_loss += loss\n        \n    avg_loss = total_loss / len(instances)\n    return avg_loss\n\nprint('finish')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.149957,"end_time":"2020-12-13T18:22:38.761635","exception":false,"start_time":"2020-12-13T18:22:38.611678","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### (TO-DO!) Let's train this beeg boy ;-)"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T18:22:39.081396Z","iopub.status.busy":"2020-12-13T18:22:39.080406Z","iopub.status.idle":"2020-12-13T19:26:10.125125Z","shell.execute_reply":"2020-12-13T19:26:10.124362Z"},"papermill":{"duration":3811.210842,"end_time":"2020-12-13T19:26:10.125242","exception":false,"start_time":"2020-12-13T18:22:38.9144","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"\npatience, best_dev_loss = 0, 1e10\nbest_state_dict = model.state_dict()\n\nstart_time = time()\ndataloader = get_train_dataloader(train_instances, batch_size=batch_size, num_workers=num_workers)\nfor epoch in range(1, max_epochs+1):\n    model.train()\n    for i, batch in enumerate(dataloader, start=1):\n        batch = (tensor.cuda() for tensor in batch)\n        input_ids, attention_mask, token_type_ids, labels = batch\n        \n        # Backpropogation\n        ''' TO-DO: \n        1. Compute the cross-entropy loss (using built-in loss of BertForMultipleChoice)\n          (Hint: You need to call a function of model which takes all the 4 tensors in the batch as inputs)\n         \n        2. Perform backpropogation on the loss (i.e. compute gradients)\n        3. Optimize the model.\n          (Hint: These two lines of codes can be found in PyTorch tutorial)\n        '''\n        model.zero_grad()\n        outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, \n                        attention_mask=attention_mask, labels=labels)\n        ### 1. insert_missing_code\n        loss = outputs[0]\n        ### 2. insert_missing_code\n        loss.backward()\n        ### 3. insert_missing_code\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n    # Progress bar with timer ;-)\n        elapsed_time = time() - start_time\n        elapsed_time = timedelta(seconds=int(elapsed_time))\n        print(\"EPOCH: %d/%d | Batch: %d/%d | loss=%.5f | %s      \\r\" \\\n              % (epoch, max_epochs, i, len(dataloader), loss, elapsed_time), end='')\n\n    # Save parameters of each epoch\n    if save_model_path is not None:\n        save_checkpoint_path = \"%s/epoch_%d\" % (save_model_path, epoch)\n        model.save_pretrained(save_checkpoint_path)\n        \n    # Get avg. loss on development set\n    print(\"Epoch: %d/%d | Validating...                           \\r\" % (epoch, max_epochs), end='')\n    dev_loss = validate(model, dev_instances)\n    elapsed_time = time() - start_time\n    elapsed_time = timedelta(seconds=int(elapsed_time))\n    print(\"Epoch: %d/%d | dev_loss=%.5f | %s                      \" \\\n          % (epoch, max_epochs, dev_loss, elapsed_time))\n    \n    # Track best checkpoint and earlystop patience\n    if dev_loss < best_dev_loss:\n        patience = 0\n        best_dev_loss = dev_loss\n        best_state_dict = deepcopy(model.state_dict())\n        if save_model_path is not None:\n            model.save_pretrained(save_model_path)\n    else:\n        patience += 1\n    \n    if patience > max_patience:\n        print('Earlystop at epoch %d' % epoch)\n        break\n        \n# Restore parameters with best loss on development set\nmodel.load_state_dict(best_state_dict)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":2.655757,"end_time":"2020-12-13T19:26:15.190358","exception":false,"start_time":"2020-12-13T19:26:12.534601","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Testing"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T19:26:19.722988Z","iopub.status.busy":"2020-12-13T19:26:19.722253Z","iopub.status.idle":"2020-12-13T19:26:19.726038Z","shell.execute_reply":"2020-12-13T19:26:19.725612Z"},"papermill":{"duration":2.260519,"end_time":"2020-12-13T19:26:19.726132","exception":false,"start_time":"2020-12-13T19:26:17.465613","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class TestingDataset(Dataset):\n    def __init__(self, instances):\n        self.instances = instances\n    \n    def __len__(self):\n        return len(self.instances)\n        \n    def __getitem__(self, i):\n        instance = self.instances[i]\n        input_ids = instance['input_ids']\n        attention_mask = instance['attention_mask']\n        token_type_ids = instance['token_type_ids']\n        input_ids = torch.LongTensor(input_ids)\n        attention_mask = torch.FloatTensor(attention_mask)\n        token_type_ids = torch.LongTensor(token_type_ids)\n        return input_ids, attention_mask, token_type_ids, \n    \ndef get_test_dataloader(instances, batch_size=8, num_workers=4):\n    def collate_fn(batch):\n        input_ids, attention_mask, token_type_ids = zip(*batch)\n        input_ids = pad_sequence(input_ids, batch_first=True).unsqueeze(1)  # predict as single choice\n        attention_mask = pad_sequence(attention_mask, batch_first=True).unsqueeze(1)\n        token_type_ids = pad_sequence(token_type_ids, batch_first=True).unsqueeze(1)\n        return input_ids, attention_mask, token_type_ids\n    \n    dataset = TestingDataset(instances)\n    dataloader = DataLoader(dataset, collate_fn=collate_fn, shuffle=False, \\\n                            batch_size=batch_size, num_workers=num_workers)\n    return dataloader","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":2.969209,"end_time":"2020-12-13T19:26:25.014914","exception":false,"start_time":"2020-12-13T19:26:22.045705","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## (TO-DO!) Define function to predict BERT scores"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T19:26:29.671091Z","iopub.status.busy":"2020-12-13T19:26:29.670343Z","iopub.status.idle":"2020-12-13T19:26:29.674165Z","shell.execute_reply":"2020-12-13T19:26:29.673343Z"},"papermill":{"duration":2.312885,"end_time":"2020-12-13T19:26:29.674273","exception":false,"start_time":"2020-12-13T19:26:27.361388","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def predict_query_doc_scores(model, df):\n    model.eval()\n    start_time = time()\n\n    # Parse CSV\n    query_id_list = df[\"query_id\"]\n    query_text_list = df[\"query_text\"]\n    bm25_top1000_list = df[\"bm25_top1000\"]\n\n    # Treat {1 query, K documents} as a dataset for prediction\n    query_doc_scores = []\n    query_doc_ids = []\n    rows = zip(query_id_list, query_text_list, bm25_top1000_list)\n    for qi, row in enumerate(rows, start=1):\n        query_id, query_text, bm25_top1000 = row\n        bm25_doc_id_list = bm25_top1000.split()\n        query_doc_ids.append(bm25_doc_id_list)\n\n        #################################################\n        #    Collect all instances of query/doc pairs\n        #################################################\n        query_instances = []\n\n        # Make query tokens for BERT\n        query_tokens = tokenizer.tokenize(query_text)\n        if len(query_tokens) > max_query_length:  # truncation\n            query_tokens = query_tokens[:max_query_length]\n        query_token_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n        query_token_ids.insert(0, tokenizer.cls_token_id)\n        query_token_ids.append(tokenizer.sep_token_id)\n\n        # Make input instances for all query/doc pairs\n        for i, doc_id in enumerate(bm25_doc_id_list, start=1):\n            if doc_id in doc_id_to_token_ids:\n                doc_token_ids = doc_id_to_token_ids[doc_id]\n            else:\n                doc_text = doc_id_to_text[doc_id]\n                doc_tokens = tokenizer.tokenize(doc_text)\n                doc_token_ids = tokenizer.convert_tokens_to_ids(doc_tokens)\n                doc_id_to_token_ids[doc_id] = doc_token_ids\n            doc_token_ids.append(tokenizer.sep_token_id)\n\n            # make input sequences for BERT\n            input_ids = query_token_ids + doc_token_ids\n            token_type_ids = [0 for token_id in query_token_ids]\n            token_type_ids.extend(1 for token_id in doc_token_ids)\n            if len(input_ids) > max_input_length:  # truncation\n                input_ids = input_ids[:max_input_length]\n                token_type_ids = token_type_ids[:max_input_length]\n            attention_mask = [1 for token_id in input_ids]\n\n            # convert and collect inputs as tensors\n            input_ids = torch.LongTensor(input_ids)\n            attention_mask = torch.FloatTensor(attention_mask)\n            token_type_ids = torch.LongTensor(token_type_ids)\n\n\n            # collect all inputs as a dictionary\n            instance = {}\n            instance['input_ids'] = input_ids\n            instance['attention_mask'] = attention_mask\n            instance['token_type_ids'] = token_type_ids\n            query_instances.append(instance)\n\n        #################################################################\n        #    Predict relevance scores for all BM25-top-1000 documents\n        #################################################################\n        doc_scores = np.empty((0,1))\n\n        # Predict scores for each document\n        dataloader = get_test_dataloader(query_instances, batch_size=batch_size*(num_negatives+1), num_workers=num_workers)\n        for di, batch in enumerate(dataloader, start=1):\n            batch = (tensor.cuda() for tensor in batch)\n            input_ids, attention_mask, token_type_ids = batch\n            \n            ''' TO-DO: \n            1. Compute the logits as relevance scores (using the same function of how you compute built-in loss)\n              (Hint: You need to call a function of model which takes all the 3 tensors in the batch as inputs)\n         \n            2. The scores are still on GPU. Reallocate them on CPU, and convert into numpy arrays.\n              (Hint: You need to call two functions on the `scores` tensors. You can find them in PyTorch tutorial.)\n            '''\n            with torch.no_grad():\n                outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, \n                            attention_mask=attention_mask)\n        \n            # Move logits and labels to CPU\n            logits = outputs[0]\n            logits = logits.detach().cpu().numpy()\n            scores = logits### 1. insert_missing_code_to_compute_logits ###\n            \n            # merge all scores into a big numpy array\n            doc_scores = np.vstack((doc_scores, scores))\n        \n            # Progress bar with timer ;-)\n            elapsed_time = time() - start_time\n            elapsed_time = timedelta(seconds=int(elapsed_time))\n            print(\"Query: %d/%d | Progress: %d/%d | %s      \\r\" \\\n                  % (qi, len(df), di, len(dataloader), elapsed_time), end='')\n\n        # merge all query/BM25 document pair scores\n        query_doc_scores.append(doc_scores)\n    query_doc_scores = np.hstack(query_doc_scores).T\n\n    print()\n    return query_doc_scores, query_doc_ids","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":2.295824,"end_time":"2020-12-13T19:26:34.25109","exception":false,"start_time":"2020-12-13T19:26:31.955266","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## (TO-DO!) Find best weight of BERT for BM25 rescoring on training set"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T19:26:39.150191Z","iopub.status.busy":"2020-12-13T19:26:39.149311Z","iopub.status.idle":"2020-12-13T19:35:50.921695Z","shell.execute_reply":"2020-12-13T19:35:50.922568Z"},"papermill":{"duration":554.067906,"end_time":"2020-12-13T19:35:50.922798","exception":false,"start_time":"2020-12-13T19:26:36.854892","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"dev_query_doc_scores, dev_query_doc_ids = predict_query_doc_scores(model, dev_df)\n\nprint('---- Grid search weight for \"BM25 + weight * BERT\" ----')\nbest_map_score, best_bert_weight = -100, 0.0\nbert_scores = dev_query_doc_scores\nn_query = dev_query_doc_scores.shape[0]\n\n# Get MAP@K of BM25 baseline\nquery_pos_doc_ids = dev_df['pos_doc_ids'].values.tolist()\nactual = [doc_ids.split() for doc_ids in query_pos_doc_ids]\nbm25_predicted = [doc_id_list[:K] for doc_id_list in dev_query_doc_ids]\nmap_score = mapk(actual, bm25_predicted, k=K)\nbest_map_score = map_score\nprint(\"weight=%.1f: %.5f  (BM25 baseline)\" % (0, 100*map_score))\n\n# Collect BM25 scores into same format of BERT scores\n''' TO-DO-finished: \n1. Convert the BM25 top-1000 scores into 2d numpy arrays\n2. BM25 scores should have the same shape and orders as `dev_query_doc_scores` (i.e. BERT scores)\n  (Hint: If there are 24 dev-set queries, the shape should be (24, 1000) )\n'''\n### insert_whatever_you_want_to_meet_the_requirement_in_step2. ###\nbm25_scores = [scores.split() for scores in dev_df[\"bm25_top1000_scores\"]]\nbm25_scores = [[float(score) for score in scores] for scores in bm25_scores]  # convert to float\nbm25_scores = np.array(bm25_scores)\nprint(bm25_scores.shape)\n# Grid search for BM25 + BERT rescoring\nlow_bound, high_bound, scale = 0, 5, 1000\ngrids = [i / scale for i in range(low_bound * scale+1, high_bound * scale+1)]\nfor weight in grids:\n    \n    ''' TO-DO: \n    1. Compute the weighted scores using `bm25_scores`, `weight`, and `bert_scores`\n    '''\n    \n    weighted_scores = bm25_scores + weight * bert_scores### 1. insert_missing_code ###\n    # sort index and map to document ids as output\n    rescore_argsort = np.flip(weighted_scores.argsort(), axis=1)\n    predicted = []\n    for i in range(n_query):  # num. of queries\n        predicted.append([dev_query_doc_ids[i][idx] for idx in rescore_argsort[i]][:K])\n    map_score = mapk(actual, predicted, k=K)\n    \n    # show part of results for human evaluation\n    if weight * 10 % 2 == 0:\n        print(\"weight=%.1f: %.5f\" % (weight, 100*map_score))\n        \n    # track weight with best MAP@10\n    if map_score > best_map_score:\n        best_map_score = map_score\n        best_bert_weight = weight\nprint(\"\\nHighest MAP@%d = %.5f found at weight=%.3f\" % (K, 100*best_map_score, best_bert_weight))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":2.945901,"end_time":"2020-12-13T19:35:56.574567","exception":false,"start_time":"2020-12-13T19:35:53.628666","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## (TO-DO!) Rescore testing set with BERT for submission"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T19:36:03.320789Z","iopub.status.busy":"2020-12-13T19:36:03.320177Z","iopub.status.idle":"2020-12-13T20:03:12.318734Z","shell.execute_reply":"2020-12-13T20:03:12.311873Z"},"papermill":{"duration":1633.031407,"end_time":"2020-12-13T20:03:12.318873","exception":false,"start_time":"2020-12-13T19:35:59.287466","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Predict BERT scores for testing set\ntest_df = pd.read_csv(testing_csv_path)\nquery_id_list = test_df[\"query_id\"]\nn_query = len(query_id_list)\ntest_query_doc_scores, test_query_doc_ids = predict_query_doc_scores(model, test_df)\nbert_scores = test_query_doc_scores","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-13T20:03:20.970101Z","iopub.status.busy":"2020-12-13T20:03:20.968845Z","iopub.status.idle":"2020-12-13T20:03:21.302169Z","shell.execute_reply":"2020-12-13T20:03:21.300971Z"},"papermill":{"duration":4.537738,"end_time":"2020-12-13T20:03:21.302306","exception":false,"start_time":"2020-12-13T20:03:16.764568","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# Rescore query/document score with BM25 + BERT\nbm25_scores = [scores.split() for scores in test_df[\"bm25_top1000_scores\"]]  # parse into 2d list of string\nbm25_scores = [[float(score) for score in scores] for scores in bm25_scores]  # convert to float\nbm25_scores = np.array(bm25_scores)\n\n''' TO-DO-finished: \n1. Compute the weighted scores using `bm25_scores`, `best_bert_weight`, and `bert_scores`\n'''\nweighted_scores = bm25_scores+ best_bert_weight*bert_scores### 1. insesrt_missing_code ###\n\n# Rerank document ids with new scores\nrescore_argsort = np.flip(weighted_scores.argsort(), axis=1)\nranked_doc_id_list = []\nfor i in range(n_query):  # num. of queries\n    ranked_doc_id_list.append([test_query_doc_ids[i][idx] for idx in rescore_argsort[i]][:K])\nranked_doc_ids = [' '.join(doc_id_list) for doc_id_list in ranked_doc_id_list]\n\n# Save reranked results for submission\ndata = {'query_id': query_id_list, 'ranked_doc_ids': ranked_doc_ids}\nsubmission_df = pd.DataFrame(data)\nsubmission_df.reset_index(drop=True, inplace=True)\nsubmission_df.to_csv(save_submission_path, index=False)\nprint(\"Saved submission file as `%s`\" % save_submission_path)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}